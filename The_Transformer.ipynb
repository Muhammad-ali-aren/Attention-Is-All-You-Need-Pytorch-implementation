{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+b4auXpqmyusYHwHMWMfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-ali-aren/Attention-Is-All-You-Need-Pytorch-implementation/blob/main/The_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  so first we make tokenizer.\n",
        "2. we need to make the multihead attention mechanism.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l88AVsH29Kjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "CWtPGdOD-eKj"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "t-YCSaereubH"
      },
      "outputs": [],
      "source": [
        "#some varible have poor names, once its completely working i will change to have proper names\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,D=512,num_heads=8,mask=False,seq_len=5):\n",
        "        super().__init__()\n",
        "        assert D % num_heads == 0, 'model dims must be divisible by number of heads'\n",
        "        self.head_dim = D // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.mask = mask\n",
        "        self.D = D\n",
        "        self.scale =self.head_dim ** 0.5\n",
        "        self.Q_linear = nn.Linear(D,D)\n",
        "        self.K_linear = nn.Linear(D,D)\n",
        "        self.V_linear = nn.Linear(D,D)\n",
        "        self.linear_out = nn.Linear(D,D)\n",
        "    def forward(self,x,out_encoder=None):\n",
        "        B,N,L = x.shape\n",
        "        Q = self.Q_linear(x)\n",
        "        K = self.K_linear(x)\n",
        "        V = self.V_linear(x)\n",
        "        if torch.is_tensor(out_encoder):\n",
        "            Q = self.Q_linear(x)\n",
        "            K = self.K_linear(out_encoder)\n",
        "            V = self.V_linear(out_encoder)\n",
        "        # Batch, sequence_length, model_dims -> Batch, num_heads, seq_length, head_dim\n",
        "        Q = Q.view(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        K = K.view(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        V = V.view(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        attention_score = torch.matmul(Q,K.transpose(-2,-1)) / self.scale\n",
        "        if self.mask:\n",
        "            mask = torch.tril(torch.ones(N,N))\n",
        "            # seq_len x seq_len\n",
        "            attention_score = attention_score.masked_fill(mask == 0,-1e9)\n",
        "        probs = nn.functional.softmax(attention_score,dim=-1)\n",
        "        out = torch.matmul(probs,V)\n",
        "        # batch,num_heads,seq_length,head_dim -> batch, seq_length,num_heads,heads_dim\n",
        "        out = out.transpose(1,2).contiguous().view(B,N,self.D)\n",
        "        out = self.linear_out(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self,D=512,dff=2048,num_heads=8,mask=False,seq_len=5):\n",
        "        super().__init__()\n",
        "        self.MHA = MultiHeadAttention(D=D,num_heads=num_heads,mask=mask,seq_len=seq_len)\n",
        "        self.LayerNorm1 = nn.LayerNorm(D)\n",
        "        self.LayerNorm2 = nn.LayerNorm(D)\n",
        "        self.FeedForward = nn.Sequential(\n",
        "            nn.Linear(D,dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff,D)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "    def forward(self,x):\n",
        "        out_mha = self.dropout(self.MHA(x))\n",
        "        res1 = self.LayerNorm1(out_mha + x)\n",
        "        out_ffd = self.dropout(self.FeedForward(res1))\n",
        "        res2 = self.LayerNorm2(out_ffd + res1)\n",
        "        return res2\n"
      ],
      "metadata": {
        "id": "dYtQoErqIrGN"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,layers=6,D=512,dff=2048,num_heads=8,mask=False,seq_len=5):\n",
        "        super().__init__()\n",
        "        self.EncoderLayer = nn.ModuleList(EncoderBlock(D=D,num_heads=num_heads,mask=mask,seq_len=seq_len) for i in range(layers))\n",
        "    def forward(self,x):\n",
        "        for layer in self.EncoderLayer:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EIjy3QQEIJeU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,D=512,ddf=2048,num_heads=8,mask=True,seq_len=5,input_feature_dim=512):\n",
        "        super().__init__()\n",
        "        self.MMHA = MultiHeadAttention(D=D,num_heads=num_heads,mask=mask,seq_len=seq_len)\n",
        "        self.MHA = MultiHeadAttention(D=D,num_heads=num_heads,mask=False,seq_len=seq_len)\n",
        "        self.LayerNorm1 = nn.LayerNorm(D)\n",
        "        self.LayerNorm2 = nn.LayerNorm(D)\n",
        "        self.LayerNorm3 = nn.LayerNorm(D)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.FeedForward = nn.Sequential(nn.Linear(D,ddf),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(ddf,D))\n",
        "    def forward(self,x,out_encoder):\n",
        "        out_mmha = self.dropout(self.MMHA(x)) #out_mmha = query and out_encoder = keys and values\n",
        "        res1 = self.LayerNorm1(out_mmha + x)\n",
        "\n",
        "        out_mha = self.MHA(res1,out_encoder)\n",
        "        res2 = self.LayerNorm2(res1 + out_mha)\n",
        "        out_FF = self.FeedForward(res2)\n",
        "        out = self.LayerNorm3(res2 + out_FF)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rBq4WymvDRRm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,D=512,layers=6,ddf=2048,num_heads=8,mask=True,seq_len=5):\n",
        "        super().__init__()\n",
        "        self.DecoderLayers = nn.ModuleList(DecoderBlock(D=D,ddf=ddf,num_heads=num_heads,mask=mask,seq_len=seq_len) for _ in range(layers))\n",
        "    def forward(self,x,out_encoder):\n",
        "        for layer in self.DecoderLayers:\n",
        "            x = layer(x,out_encoder)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6-XKwbNm6_Op"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,D=512,layers=6,dff=2048,num_heads=8,mask=True,seq_len=5):\n",
        "        super().__init__()\n",
        "        self.D = D\n",
        "        self.layers = layers\n",
        "        self.dff = dff\n",
        "        self.num_heads = num_heads\n",
        "        self.mask = mask\n",
        "        self.seq_len = seq_len\n",
        "        self.Embeddings = nn.Embedding(num_embeddings=1200,embedding_dim=512) #num_embedding is size of your vocabolary\n",
        "        self.Encoder = Encoder(layers,D,dff,num_heads,seq_len)\n",
        "        self.Decoder = Decoder(D,layers,dff,num_heads,mask,seq_len)\n",
        "        self.linear_layer = nn.Linear(D,D)\n",
        "    def forward(self,x):\n",
        "        x_in = x[:,:-1]\n",
        "        out_embed = self.Embeddings(x_in)\n",
        "        out_encoder = self.Encoder(out_embed)\n",
        "        # we will shift the input sentences to the right by one token\n",
        "        out_embed_dec= self.Embeddings(x[:,1:])\n",
        "        out_decoder = self.Decoder(out_embed_dec,out_encoder)\n",
        "        out = nn.functional.softmax(self.linear_layer(out_decoder),dim=-1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "598fy6EnCW0z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.randint(1,1200,(2,6))\n",
        "print(tokens)\n",
        "# out_encoder = None #torch.randn(2,5,512)\n",
        "# print(tokens)"
      ],
      "metadata": {
        "id": "-AuCtP-x_N8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b0a8da-0c48-484b-e2bc-9a187dac977b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 819,  495,  395,  427,  875,  816],\n",
            "        [1036, 1127,  976, 1149,   40, 1112]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers = 6\n",
        "\n",
        "encoder = Transformer(layers=layers)\n",
        "print(encoder(tokens).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sj9RCIS_fv0",
        "outputId": "8582efa0-d7ab-41d3-8497-3aae846e8cf9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 512])\n"
          ]
        }
      ]
    }
  ]
}