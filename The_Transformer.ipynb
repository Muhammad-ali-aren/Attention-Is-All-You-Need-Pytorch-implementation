{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  so first we make tokenizer.\n",
        "2. we need to make the multihead attention mechanism.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l88AVsH29Kjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "CWtPGdOD-eKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-YCSaereubH"
      },
      "outputs": [],
      "source": [
        "#some varible have poor names, once its completely working i will change to have proper names\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,D=512,num_heads=8,mask=False,seq_len=5,input_feature_dim=10):\n",
        "        super().__init__()\n",
        "        assert D % num_heads == 0, 'model dims must be divisible by number of heads'\n",
        "        self.head_dim = D // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.mask = mask\n",
        "        self.D = D\n",
        "        self.scale =self.head_dim ** 0.5\n",
        "        self.Q_linear = nn.Linear(input_feature_dim,D)\n",
        "        self.K_linear = nn.Linear(input_feature_dim,D)\n",
        "        self.V_linear = nn.Linear(input_feature_dim,D)\n",
        "        self.linear_out = nn.Linear(D,D)\n",
        "    def forward(self,x):\n",
        "        B,N,L = x.shape\n",
        "        Q = self.Q_linear(x)\n",
        "        K = self.K_linear(x)\n",
        "        V = self.V_linear(x)\n",
        "        # Batch, sequence_length, model_dims -> Batch, num_heads, seq_length, head_dim\n",
        "        Q = Q.view(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        K = K.view(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        V = V.view(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
        "        attention_score = torch.matmul(Q,K.transpose(-2,-1)) / self.scale\n",
        "        if self.mask:\n",
        "            mask = torch.tril(torch.ones(N,N))\n",
        "            # seq_len x seq_len\n",
        "            attention_score = attention_score.masked_fill(mask == 0,-1e9)\n",
        "        probs = nn.functional.softmax(attention_score,dim=-1)\n",
        "        out = torch.matmul(probs,V)\n",
        "        # batch,num_heads,seq_length,head_dim -> batch, seq_length,num_heads,heads_dim\n",
        "        out = out.transpose(1,2).contiguous().view(B,N,self.D)\n",
        "        out = self.linear_out(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,D=512,dff=1042,num_heads=8,mask=False,seq_len=5,input_feature_dim=10):\n",
        "        super().__init__()\n",
        "        self.MHA = MultiHeadAttention(D=D,num_heads=num_heads,mask=mask,seq_len=seq_len,input_feauture_dim=input_feature_dim)\n",
        "        self.LayerNorm1 = nn.LayerNorm(D)\n",
        "        self.LayerNorm2 = nn.LayerNorm(D)\n",
        "        self.FeedForward = nn.Sequential(\n",
        "            nn.\n",
        "        )"
      ],
      "metadata": {
        "id": "dYtQoErqIrGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.randn(2,5,10)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "-AuCtP-x_N8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = Attention(mask=True)\n",
        "print(attention(tokens).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sj9RCIS_fv0",
        "outputId": "dedbf154-6b86-4b2c-d084-925973e73a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 512])\n"
          ]
        }
      ]
    }
  ]
}